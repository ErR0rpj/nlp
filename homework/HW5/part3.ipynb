{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 5\n",
    "Created by Prof. [Mohammad M. Ghassemi](https://ghassemi.xyz)\n",
    "\n",
    "Submitted by: <span style=\"color:red\"> INSERT YOUR NAME HERE </span>\n",
    "\n",
    "In collaboration with: <span style=\"color:red\"> INSERT YOUR (OPTIONAL) HOMEWORK PARTNER'S NAME HERE </span>\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from materials.code import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# IMPORT SOME BASIC TOOLS:\n",
    "from pprint import pprint\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 3: Transformers\n",
    "If you understand how the basic Transformer model worked from the lecture, then you are equipped to understand nearly all of the variants of Transformer-based NLP. Huggingface does a good job of implementing many contemporary [Transformers in PyTorch](https://huggingface.co/transformers/) and even provides links to the papers for the transformer models. \n",
    "\n",
    "<br> In practice, training these models requires very large datasets and computational power far beyond what is tenable in our course, and even in many academic research labs! For this reason, these models tend to be used as a starting point, and are fine tuned for specific problems. That is, we use the weights of the model trained by Google, Facebook, or some other rich entity with TPU arrays as a starting point, but then we tune those weights using some new data that we have for a task we care about. \n",
    "\n",
    "<br> Because the pace of NLP research is so fast, there are many models to choose from and the landscape is continuously changing. So, in this section I would like you to learn how to navigate the NLP literature to become familiar with the many flavors of transformer models, and develop some intuition for their differences, and how to tune them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 3: \n",
    "#### Worth 3/5 Points\n",
    "#### A. Summarize contemporary transformers\n",
    "For each of the 32 [Transformer models referenced here](https://huggingface.co/transformers/), provide a brief description of the model's approach, and any advantages or disadvantages of the model. Here's an example of what you might provide for the [ALBERT](https://arxiv.org/abs/1909.11942) model.\n",
    "\n",
    "\n",
    "1. __ALBERT__:\n",
    "  * __Approach__: 2 design changes over BERT - \n",
    "    * (1) Input-level embeddings are context-independent, and hidden-layer embeddings are context-dependent allowing the embedding matrix to be relatively-low dimensional compared to prior approaches; \n",
    "    * (2) parameters are shared across layers to reduce number of parameters.\n",
    "  * __Advantage__: almost 90% reduction in the number of model parameters compared to the BERT-base model, \n",
    "  * __Disadvantage__: small reduction in model performance across reported benchmarks\n",
    "\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT WRITE UP HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "#### B. Tune a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skeleton code below as a starting point, tune the `BertSequenceModel` for a problem of your choice. If you need data, you can take take a look at the `load_data` function from earlier in the tutorial. Compare the performance of the tuned model against the un-tuned model. Comment on any differences that exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  ='./results',    # output directory\n",
    "    num_train_epochs            =3,              # total # of training epochs\n",
    "    per_device_train_batch_size =16,             # batch size per device during training\n",
    "    per_device_eval_batch_size  =64,             # batch size for evaluation\n",
    "    warmup_steps                =500,            # number of warmup steps for learning rate scheduler\n",
    "    weight_decay                =0.01,           # strength of weight decay\n",
    "    logging_dir                 ='./logs',       # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model         =model,               # the instantiated Transformers model to be trained\n",
    "    args          =training_args,       # training arguments, defined above\n",
    "    train_dataset =train_dataset,       # training dataset\n",
    "    eval_dataset  =test_dataset         # evaluation dataset\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise: \n",
    "    * <span style=\"color:red\">X</span>/3 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
