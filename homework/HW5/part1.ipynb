{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 5\n",
    "Created by Prof. [Mohammad M. Ghassemi](https://ghassemi.xyz)\n",
    "\n",
    "Submitted by: <span style=\"color:red\"> INSERT YOUR NAME HERE </span>\n",
    "\n",
    "In collaboration with: <span style=\"color:red\"> INSERT YOUR (OPTIONAL) HOMEWORK PARTNER'S NAME HERE </span>\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Goals\n",
    "The goal of this assignment is to familiarize yourself with:\n",
    "\n",
    "1. Data-driven Tokenization\n",
    "3. Seq2seq with Attention\n",
    "4. Transformers\n",
    "\n",
    "The assignment combines tutorial components, with learning exercises that you must complete and submit. The learning exercise sections are clearly demarcated within the assignments.\n",
    "\n",
    "## Before you start\n",
    "1. PULL THE LATEST VERSION OF THE `course-materials` REPOSITORY, AND COPY `homework/HW5/` INTO THE CORRESPONDING DIRECTORY OF YOUR SUBMISSION FOLDER\n",
    "2. CREATE AND ATTACH TO A VIRTUAL ENVIRONMENT, AND INSTALL THE REQUIREMENTS IN `requirements.txt`\n",
    "3. IMPORT THE COURSE UTILITIES AND RELEVANT LIBRARIES BY RUNNING THE CODE BLOCK BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from materials.code import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# IMPORT SOME BASIC TOOLS:\n",
    "from pprint import pprint\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 0: Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Common Crawl](https://commoncrawl.org/) is a non-profit organization that freely provides web crawler data in [WARC format](http://fileformats.archiveteam.org/wiki/WARC); this is useful for budding NLP researchers (and open-source Google competitors) because crawling the web at any kind of meaningful scale is challenging. If you want to add \"processed the entire internet - literally\" to your list of resume accomplishments without touching any WARC files, you can take a look at the [Oscar Corpus](https://oscar-corpus.com/) which provides a (somewhat) pre-processed version of the internet archives. \n",
    "\n",
    "I did consider doing an assignment asking you to process the entire web and whist the thought of your machine's suffering was amusing, my better nature compelled me to use a humble subset of our wonderful world wide web - Wikipedia. More specifically, we'll be using the [WikiText language modeling dataset](https://huggingface.co/nlp/viewer/?dataset=wikitext&config=wikitext-103-raw-v1), a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. Let's being by pulling the `wikitext` data from the web, and saving it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/ghamut/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------\n",
    "# Import the wikitext dataset:\n",
    "#-------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "dataset   = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Flatten out the dataset into a list of sentences and outcome, y\n",
    "#-------------------------------------------------\n",
    "sentences = dataset['train']['text']  + dataset['validation']['text'] + dataset['test']['text']\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Store the Wikipedia Data\n",
    "#-------------------------------------------------\n",
    "f = open(\"materials/data/wikitext.txt\", \"w\")\n",
    "f.write(''.join(dataset['train']['text']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>The wikipedia data subset that is provided by HuggingFace (found at `materials/data/wikitext.txt`) is a humble half a Gigabyte - rather small by NLP standards, but it will do for our tutorial's purposes. By the way, you can also download the full [daily wikipedia dumps here](https://dumps.wikimedia.org/enwiki/) in case that's something of interest to you.  \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Part 1: Data-Driven Tokenization\n",
    "At the start of this course, we covered the topic of tokenization. As you may recall, the traditional tokenization approaches use hand-crafted rules and human knowledge to separate our text into structured lists of lists which are eventually converted into tensors. In all of the assignments to-date, we've used the traditional approach to tokenization via `nltk` as a foundational pre-processing step. However, as you probably recall from your first homework assignment, there are also data-driven approaches to tokenization; on of the approaches we discussed was `Byte Pair Encoding` (BPE). \n",
    "\n",
    "The reason we've ignored BPE in favor of the traditional approaches is because, up until now, the data we've been working with has been rather small and data-driven approaches require lots of data (shocking, I know). My hope is that 500MB of `wikitext` data is sufficient that BPE might now start to yield more sensible tokenization results. \n",
    "\n",
    "Below I've provided a pre-processing and tokenization pipeline that we will use to train BPE on the wikipedia corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Libraries that will allow for the \n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers import BertWordPieceTokenizer, CharBPETokenizer, ByteLevelBPETokenizer, SentencePieceBPETokenizer\n",
    "from tokenizers.normalizers    import Lowercase, NFD, Sequence, StripAccents\n",
    "from tokenizers.processors     import BertProcessing\n",
    "from tokenizers.pre_tokenizers import Punctuation, Digits, ByteLevel\n",
    "#-------------------------------------------------\n",
    "# Training data for the Tokenizer\n",
    "#-------------------------------------------------\n",
    "training_data = ['materials/data/wikitext.txt']    \n",
    "\n",
    "#-------------------------------------------------\n",
    "# Type of Tokenizer\n",
    "#-------------------------------------------------\n",
    "tokenizer     = ByteLevelBPETokenizer()\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Pre-Tokenizers\n",
    "#-------------------------------------------------\n",
    "tokenizer.pre_tokenizer=  [ Punctuation(),         # Split on Punctation\n",
    "                            Digits(),              # Split on Digits\n",
    "                           ]\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Text normalization approach\n",
    "#-------------------------------------------------\n",
    "tokenizer.normalizer = Sequence([ NFD(),           # Fix potential unicode problems\n",
    "                                  StripAccents(),  # Remove Accents \n",
    "                                  Lowercase()      # Cast the text to lowercase\n",
    "                                ])\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Specifiy Special Tokens that are not in the text\n",
    "#-------------------------------------------------\n",
    "special_tokens = [\"<s>\",      # indicates start of text block\n",
    "                  \"<pad>\",    # padding for tensors \n",
    "                  \"</s>\",     # indicates end of a text block\n",
    "                  \"<unk>\",    # indicates out-of-vocabulary token\n",
    "                  \"<mask>\"]   # will be used to artifically \"corrupt\" data when training.  \n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Train the tokenizer\n",
    "#-------------------------------------------------\n",
    "tokenizer.train(files          = training_data,                            \n",
    "                vocab_size     = 50000, \n",
    "                min_frequency  = 2, \n",
    "                special_tokens = special_tokens,\n",
    "                show_progress  = True)\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Save the tokenizer for later use too.\n",
    "#-------------------------------------------------\n",
    "tokenizer.save_model(\"materials/tokenizers\", \"bpe.tokenizer.50k.json\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Add a post processor \n",
    "#-------------------------------------------------\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "                                                    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "                                                    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "                                                    )\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>This tokenizer is now ready to use. Also, the tokenizer conveniently creates two files for us: \n",
    "\n",
    "1. `materials/tokenizers/bpe.tokenizer.50k.json-vocab.json` contains the vocabulary that was generated by BPE; it is ranked by frequency.\n",
    "2. `materials/tokenizers/bpe.tokenizer.50k.json-merges.txt` contains the merges followed to perform the tokenization\n",
    "\n",
    "<br>These two files contain everything we need to use the tokenizer without having the train the model again. Let's apply the model to encode, and then decode a sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['<s>', 'I', 'Ġdidn', \"'\", 't', 'Ġwin', 'Ġa', 'Ġ$', '2', ',', '000', 'Ġtrip', 'Ġto', 'ĠNew', 'ĠYork', '!', '!', 'Ġ', 'ð', 'Ł', 'ĺ', 'Ń', '</s>']\n",
      "Word ids:       [0, 45, 5271, 11, 88, 1197, 263, 1136, 22, 16, 17371, 6015, 294, 762, 1252, 5, 5, 225, 177, 258, 251, 260, 2]\n",
      "Decoded string: I didn't win a $2,000 trip to New York!! 😭\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------\n",
    "# And then encode:\n",
    "#-------------------------------------------------\n",
    "encoded = tokenizer.encode(\"I didn't win a $2,000 trip to New York!! 😭\")\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoded.tokens))\n",
    "print(\"Word ids:       {}\".format(encoded.ids))\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> A couple of things I want to point out about these results:\n",
    "\n",
    "1. Note that many of the tokens start with the `Ġ` character; this is a representation of whitespace when things are computed at the Byte-level with BPE. \n",
    "2. Also note that the data-driven approach here decided to split `didn't` into `didn`, `'`, and `t`. This may not be the best tokenization approach for all problems, but the advantage of techniques like BPE is that they do a great job for tokenization without any prior human knowledge about the problem domain. \n",
    "3. Note that even though our training data didn't contain a 😭, the tokenizer still created a representation: `['Ġ', 'ð', 'Ł', 'ĺ', 'Ń']`.\n",
    "\n",
    "Personally, I prefer to use [Spacy](https://spacy.io/usage/linguistic-features#how-tokenizer-works) or even just simple `nltk` for smaller tokenization projects, and BPE for larger tokenization projects, or projects whrere I'm dealing with a text modality I'm unfamiliar with (e.g. A language I don't know). You can [read more here](https://blog.floydhub.com/tokenization-nlp/#unigram) about various tokenization approaches, as well as their pros/cons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained tokenizers\n",
    "\n",
    "Because we saved our vocabulary and merges to disk, we can load them instead of re-training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'A',\n",
       " 'Ġjourney',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġcenter',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġearth',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors      import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\"materials/tokenizers/bpe.tokenizer.50k.json-vocab.json\",\n",
    "                                  \"materials/tokenizers/bpe.tokenizer.50k.json-merges.txt\")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing((\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "                                                     (\"<s>\", tokenizer.token_to_id(\"<s>\")))\n",
    "\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode(\"A journey to the center of the earth.\")\n",
    "encoded.tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We can also download pre-trained tokenizers used by contemporary machine learning models, such as OpenAI's [GPT2](https://openai.com/blog/better-language-models/) by downloading the [gpt2-vocab.json](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json) and [gpt2-merges.txt](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt) files; I'm bringing this up because new models will (sometimes) make these files available, and if someone trained a tokenizer in a domain that's related to yours, it might save you some time to use it.\n",
    "\n",
    "<br> I should also note that the tokenizers used by many of the more popular NLP models (e.g. [BERT](https://arxiv.org/pdf/1810.04805.pdf), [XLNet](https://arxiv.org/pdf/1906.08237.pdf)), are freely available from the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'didn', \"'\", 't', 'win', 'a', '$', '2', ',', '000', 'trip', 'to', 'New', 'York', '!', '!', '[UNK]']\n",
      "['▁I', '▁didn', \"'\", 't', '▁win', '▁a', '▁$2,000', '▁trip', '▁to', '▁New', '▁York', '!!', '▁', '😭']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, XLNetTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')    # WordPiece based\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased') # SentencePiece based\n",
    "\n",
    "tokenized = bert_tokenizer.tokenize(\"I didn't win a $2,000 trip to New York!! 😭\")\n",
    "print(tokenized)\n",
    "\n",
    "tokenized = xlnet_tokenizer.tokenize(\"I didn't win a $2,000 trip to New York!! 😭\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 1: \n",
    "#### Worth 1/5 Points\n",
    "#### A. The Unigram Tokenization Approach\n",
    "Implement the unigram tokenization approach (from scratch) described [in this paper](https://arxiv.org/pdf/1804.10959.pdf). Demonstrate the tokenization on the wikipedia data. Comment on the advantages of this approach over BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise: \n",
    "    * <span style=\"color:red\">X</span>/1 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
