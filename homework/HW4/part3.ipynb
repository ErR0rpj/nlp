{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Batching\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "from materials.code import utils\n",
    "importlib.reload(utils)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# IMPORT SOME BASIC TOOLS:\n",
    "from pprint import pprint\n",
    "import pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing Data\n",
    "In the previous two portions of the tutorial (and the previous assignments for that matter) we've followed a standard procedure when we want to use text data for classification:\n",
    "\n",
    "0. Load our text data into memory.\n",
    "1. Format the text data to a set of tokenized sentences (e.g. list of lists). \n",
    "2. Format the labels we want to predict as a list. (e.g. \"Positive\" or \"Negative\")\n",
    "3. Generate a numerical representation of the text data (e.g. bag-of-words, or word vectors).\n",
    "4. Generate a numerical representation of the of the labels (e.g. Positive = `1`, Negative = `0`). \n",
    "5. Train a model that maps the numerical representation of the text to the numerical representation of the labels.\n",
    "\n",
    "This procedure is going to be roughly the same no matter what kind of data we're working with, or what kind of labels we want to predict. But one fatal flaw in our execution of this general procedure has been the foundation \"Step 0\" - where we've been loading **our entire dataset into memory, all at once** for tokenization, formating, splitting into training and testing sets, modeling etc.  \n",
    "\n",
    "We've been able to get away with this because our datasets have (intentionally) been on the smaller side; the Rotten Tomatoes movie dataset, for instance, was a puny 10,000 reviews. But when you are training models in the real-world, it's often not practical (or even possible) to pull the entire dataset into memory at once. Perhaps you faced a painful memory error in the previous learning exercises if you dared not to store the data in sparse array or dict - **and that data wasn't even too large by NLP standards!** Imagine if you had to represent all of wikipedia as a bag of words, or train a model to predict poorly written Wikipedia articles. \n",
    "\n",
    "One solution is to quit graduate school, and go work for a company with giant super-computers; another (and I think better) solution is to find ways to do your processing in manageable chunks, or **batches**. Let's go through some practical examples of batching with a larger version of the rotten tomatoes movie review dataset that I [downloaded from the web](https://drive.google.com/file/d/1N8WCMci_jpDHwCVgSED-B9yts-q9_Bb5/view). You can see the data at `materials/data/rotten_tomatoes_reviews_raw.csv`. This version has 480,000 reviews so it's getting to the point that it might be painful to do any kind of advanced processing with it.  \n",
    "\n",
    "So, instead of processing the data all at once, let's open the data file and read in only the first three lines, each time placing (and replacing) what see in the variable `line`, so that we don't abuse our poor computer's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1: Freshness,Review\n",
      "\n",
      "Line 2: 1,\" Manakamana doesn't answer any questions, yet makes its point: Nepal, like the rest of our planet, is a picturesque but far from peaceable kingdom.\"\n",
      "\n",
      "Line 3: 1,\" Wilfully offensive and powered by a chest-thumping machismo, but it's good clean fun.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Location of the rotten tomatoes review data.\n",
    "data_path = 'materials/data/rotten_tomatoes_reviews_raw.csv'\n",
    "\n",
    "# Open the data file\n",
    "with open(data_path) as fp:\n",
    "    \n",
    "    # read the first line and print\n",
    "    line = fp.readline()\n",
    "    print('Line 1:', line)\n",
    "    \n",
    "    # read the second line and print\n",
    "    line = fp.readline()\n",
    "    print('Line 2:', line)\n",
    "\n",
    "    # read the third line and print\n",
    "    line = fp.readline()\n",
    "    print('Line 3:', line)\n",
    "    \n",
    "    # Shall i go on?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> As we can tell from line 1, this is a simple CSV file where the first element indicates the `Freshness` (1 = positive), and the second entry indicates the free text `Review`. If we were to parse this file one line at a time, we'll probably want to split it on the `,` character, but note from the above examples that there are commas all over the reviews too! So, this will make splitting the text data up neatly (with `.split(',')` for instance) a little less straight forward.\n",
    "\n",
    "If you are collecting a custom dataset, or working with publicly available sources, you will probably have to work with data that is in CSV format at some point. However, especially for text data, it is often useful to convert your data to JSON lines format, which [has several advantages over CSV format](https://jsonlines.org/examples/), especially when dealing with sparse datasets.\n",
    "\n",
    "So without futher delay, let's write a memory-friendly function that will convert the larger Rotton Tomatoes movie review data from a CSV file to JSON lines format file. Let's do 100,000 lines at a time, so I'll set the `batch_size = 100,000`. Note that the code for the `csvToJsonl` function shown below is in `utils.py` in case you want to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 0 :  100001 / 480001 Lines\n",
      "Processed Batch 1 :  200001 / 480001 Lines\n",
      "Processed Batch 2 :  300001 / 480001 Lines\n",
      "Processed Batch 3 :  400001 / 480001 Lines\n",
      "Processed Batch 4 :  480001 / 480001 Lines\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "utils.csvToJsonl(source_file      = 'materials/data/rotten_tomatoes_reviews_raw.csv', \n",
    "                 destination_file = 'materials/data/rt_reviews/re_reviews.jsonl', \n",
    "                 batch_size       = 100000, \n",
    "                 verbose          = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> The file had a size of 480,000 data rows (+1 header row) - and we wanted to convert the data in batches of 100,000 rows - so we completed the processing in 5 batches - as shown in the handy printout above.\n",
    "\n",
    "<br> So now we have our data in `jonsl` format, but how are we going to use it to train a model? Well, if we're going to train a neural network (or some other) classifier, we don't *have to* update our model parameters using all the data at once. Instead, we can update our model parameters using a strategic **subset** of the data; that is, we can train the model in multiple **batches** over multiple epochs, and given enough batches+epochs, we should be able to converge to a setting of the parameters that is close to what we would have obtained if the batch size was as large as the entire dataset! \n",
    "\n",
    "If we're going to train our models using batches, we'll need a function that will pull only the batch into memory so we can leave the rest of the data on the disk until we need it! To help with this, I've written a function that gets a batch from the `.jsonl` data we just generated (you can find the code for this in the `/materials/code/utils.py`). Let's use this function to get `batch_number` 89, assuming a `batch_size` of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Freshness': '0',\n",
      "  'Review': 'Only two scenes do more than hint at the poetic potential of the '\n",
      "            'premise.'},\n",
      " {'Freshness': '0',\n",
      "  'Review': \"[T]he movie's inability to make up its mind on an approach sinks \"\n",
      "            'it.'},\n",
      " {'Freshness': '0',\n",
      "  'Review': 'Hiistorical events referenced in the first two installments of '\n",
      "            'the Underworld franchise are brought to life in this passionless, '\n",
      "            'idea-deprived prequel.\"'},\n",
      " {'Freshness': '1',\n",
      "  'Review': 'Incendiary material treated in a (thankfully) non-incendiary '\n",
      "            'manner.'},\n",
      " {'Freshness': '1',\n",
      "  'Review': \"Hand the Oscar to Jeff Bridges right now, and let's be done with \"\n",
      "            'it.\"'}]\n"
     ]
    }
   ],
   "source": [
    "batch, end_flag = utils.getBatch(data_path    = 'materials/data/rt_reviews/re_reviews.jsonl', \n",
    "                                      batch_size   = 5,\n",
    "                                      batch_number = 89, \n",
    "                                      random_seed  = 10)\n",
    "\n",
    "# Print out the batch\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> As we can see, we get 5 rows of json style data back. Here's another example of how to pull the first and the last batches of size 100,000 from the `.jsonl` file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Size of the first batch (batch \"0\") o is: 100000\n",
      "Was this the final batch: False\n",
      "\n",
      "The Size of the fifth batch (batch \"4\") is: 80000\n",
      "Was this the final batch: True\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Read in the first batch of the data, assuming a batch size of 100,000\n",
    "#-----------------------------------------------------\n",
    "batch, end_flag = utils.getBatch(data_path    = 'materials/data/rt_reviews/re_reviews.jsonl', \n",
    "                                      total_lines  = 480000,\n",
    "                                      batch_size   = 100000,\n",
    "                                      batch_number = 0, \n",
    "                                       random_seed  = 1)\n",
    "\n",
    "print('The Size of the first batch (batch \"0\") o is:', len(batch))\n",
    "print('Was this the final batch:', end_flag)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Read in the fifth batch of the data, assuming a batch size of 100,000\n",
    "#-----------------------------------------------------\n",
    "batch, end_flag = utils.getBatch(data_path    = 'materials/data/rt_reviews/re_reviews.jsonl', \n",
    "                                      total_lines  = 480000,\n",
    "                                      batch_size   = 100000,\n",
    "                                      batch_number = 4, \n",
    "                                      random_seed  = 1)\n",
    "\n",
    "print('\\nThe Size of the fifth batch (batch \"4\") is:',len(batch))\n",
    "print('Was this the final batch:', end_flag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> Notice that the size of the 4th and final batch is less than 100,000 in size - this is because the data is not a perfect multiple of 100,000 (it's 480,000, remember?). The function contains a helpful indicator that let's us know when we've reached the final batch - the `end_flag`.\n",
    "\n",
    "So now we have `jsonl` data, and we can collect batches from it but we're not done yet. We still need to generate a `training`, `validation`, and `testing` set. In the spirit of protecting our precious memory - let's generate those datasets in batches as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing:  materials/data/rt_reviews/train_re_reviews.jsonl\n",
      "Initializing:  materials/data/rt_reviews/validation_re_reviews.jsonl\n",
      "Initializing:  materials/data/rt_reviews/test_re_reviews.jsonl\n",
      "Processed Batch 1 :  100000 / 480000 Lines\n",
      "Processed Batch 2 :  200000 / 480000 Lines\n",
      "Processed Batch 3 :  300000 / 480000 Lines\n",
      "Processed Batch 4 :  400000 / 480000 Lines\n",
      "Processed Batch 5 :  480000 / 480000 Lines\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "utils.splitFile( file       = 'materials/data/rt_reviews/re_reviews.jsonl',\n",
    "                 splits     = {'train'     :{'percentage':60},\n",
    "                               'validation':{'percentage':20},\n",
    "                               'test'      :{'percentage':20}},\n",
    "                batch_size  = 100000,\n",
    "                random_seed = 99,\n",
    "                verbose     = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> The `splitFile` function (again, available for your review in `utils`) splits the data into batches according to the arguments provided in the `splits` dictionary. Importantly, we don't have to call the partitions of the data `train`, `validation` and `test` - we can call it whatever we want, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing:  materials/data/rt_reviews/1_re_reviews.jsonl\n",
      "Initializing:  materials/data/rt_reviews/2_re_reviews.jsonl\n",
      "Initializing:  materials/data/rt_reviews/3_re_reviews.jsonl\n",
      "Initializing:  materials/data/rt_reviews/4_re_reviews.jsonl\n",
      "Processed Batch 1 :  50000 / 480000 Lines\n",
      "Processed Batch 2 :  100000 / 480000 Lines\n",
      "Processed Batch 3 :  150000 / 480000 Lines\n",
      "Processed Batch 4 :  200000 / 480000 Lines\n",
      "Processed Batch 5 :  250000 / 480000 Lines\n",
      "Processed Batch 6 :  300000 / 480000 Lines\n",
      "Processed Batch 7 :  350000 / 480000 Lines\n",
      "Processed Batch 8 :  400000 / 480000 Lines\n",
      "Processed Batch 9 :  450000 / 480000 Lines\n",
      "Processed Batch 10 :  480000 / 480000 Lines\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "utils.splitFile( file       = 'materials/data/rt_reviews/re_reviews.jsonl',\n",
    "                 splits     = {'1'     :{'percentage':25},\n",
    "                               '2'     :{'percentage':25},\n",
    "                               '3'     :{'percentage':25},\n",
    "                               '4'     :{'percentage':25}\n",
    "                              },\n",
    "                batch_size  = 50000,\n",
    "                random_seed = 99,\n",
    "                verbose     = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 3: \n",
    "#### Worth 1/5 Points\n",
    "#### A. Generating Vocabulary in Batches\n",
    "The following function was written to naively extract the vocabulary (i.e. the set of distinct tokens) from the larger Rotten Tomatoes dataset in `materials/data/rt_reviews/re_reviews.jsonl`. The function reads in the file one line at a time, appends all the data together, does some simple pre-processing and then saves the tokens and their counts in a regular json (not jsonl) file `vocabulary.json`. Please update the code block below so that the vocabulary and the frequency count of each word is generated in batches; that is, we would like to obtain the same vocabulary and word counts but without loading everything into memory! \n",
    "\n",
    "After you implemented your function, compare it's memory usage and run time against the original implementation below (if you're using Unix, you can see memory use by running `top` in the command line). Comment on any differences you see, and discuss why those differences might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re \n",
    "  \n",
    "#Counts the frequency of terms in a list\n",
    "def countFrequency(my_list): \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "    return freq\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Inputs\n",
    "#-----------------------------------------------------\n",
    "file, text_key    = 'materials/data/rt_reviews/re_reviews.jsonl', 'Review'\n",
    "batch_size        = 100000\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# House-keeping variables\n",
    "#-----------------------------------------------------\n",
    "line, vocabulary  = True, set([])\n",
    "reviews           = ''\n",
    "line_num          = 0\n",
    "n                 = utils.file_len(file)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Reading in the data\n",
    "#-----------------------------------------------------\n",
    "with open(file) as read_file: \n",
    "    while line:   \n",
    "\n",
    "        # If this is the last line, break\n",
    "        if line_num == n:\n",
    "            break\n",
    "        \n",
    "        # Read the line\n",
    "        line               = read_file.readline()\n",
    "        \n",
    "        # Process the line\n",
    "        try:\n",
    "            processed_line     = json.loads(line)\n",
    "        except:\n",
    "            print(line)\n",
    "        \n",
    "        # Append to the reviews\n",
    "        reviews           += ' ' + processed_line[text_key]\n",
    "        \n",
    "\n",
    "#-----------------------------------------------------        \n",
    "# Some Very Simple Pre-processing\n",
    "#-----------------------------------------------------\n",
    "# Basic Tokenization with NLTK\n",
    "vocabulary = list(nltk.word_tokenize(gensim.utils.to_unicode(reviews.lower())))       \n",
    "\n",
    "# Counting Frequency\n",
    "vocabulary = countFrequency(vocabulary)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Save the results as a regular JSON, not json lines\n",
    "#-----------------------------------------------------\n",
    "dirname, filename  = '/'.join(file.split('/')[:-1]), file.split('/')[-1]       \n",
    "savedir            = dirname + '/' + 'vocabulary.json'\n",
    "\n",
    "with open(savedir, 'w') as outfile:\n",
    "    json.dump(vocabulary, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise: \n",
    "    * <span style=\"color:red\">X</span>/1 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
