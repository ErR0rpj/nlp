{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Recurrent Neural Networks\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from materials.code import utils\n",
    "importlib.reload(utils)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "\n",
    "# IMPORT SOME BASIC TOOLS:\n",
    "from pprint import pprint\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Introduction\n",
    "The problem with `doc2vec` is that it completely ignores the order of the words in the text when computing the document representations. In reality, the sequential order in which words appear can have an important consequence for their combined meaning. Consider, for example, if we were interested in classifying a movie review with the tokens `well`, `done`, `not`, and `bad`. It is not just the tokens, but also their order, that would be indicative of the class:\n",
    "\n",
    "> **Well done, not bad.** vs. **done bad, not well.** \n",
    "\n",
    "We need a way to account for the sequential order of words when we are performing our modeling - this is where Recurrent Neural Networks (RNNs) come into play. As we discussed in the lectures, RNNs (and their derivatives) provide a way to account for the sequential relationships between words by retaining some information about previous elements in a sequence, when considering a given element. Please note that in this tutorial, we'll mainly be covering the practical elements of how to build RNN-style networks in Pytorch. If you are interested in learning how to build a Recurrent Neural Network from scratch there is an excellent [publicly available tutorial here](https://github.com/pangolulu/rnn-from-scratch). \n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "Before we dive into building an RNN, recall that the motivating purpose of the $sigmoid$ function in the earlier tutorials was to \"squeeze\" the values of our linear functions within a range of interest. For instance, the range of 0-1. The sigmoid accomplished this nicely, but there is nothing that *requires us* to use the sigmoid is we want to restrict our output between a numerical range; indeed, there are many other functions that can accomplish this. One obvious way is to simply \"rectify\" our function's output value anytime it's out of range. For instance, if we have a function $f(x)$ that we want to bound in the range of 0-1, then we could set all values above 1 to 1, and all less than 0 to 0. If the $x$ in our $f(x)$ was generated by a linear function of some kind, then we would call $f(x)$ a rectified linear unit (or $ReLU$, for short). \n",
    "\n",
    "Both the $ReLU$ and the $sigmoid$ have outputs that are non-negative. This is fine for modeling probabilities, but there could be circumstances where we want our function to provide both positive and negative numbers; for instance, values between between -1 and 1. This can be accomplished using the $tanh$ function (or also by shifting a scaling a sigmoid or ReLU!). In the context of Neural networks, the $sigmoid$, the $ReLU$ and the $tanh$ are called **activation functions**. Let's code up each of these functions and plot them below to solidify your intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (16,6))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# A Linear Function\n",
    "#-------------------------------------------------\n",
    "def linear(X,beta):\n",
    "    return np.dot(X, beta[1:]) + beta[0]\n",
    "\n",
    "#-------------------------------------------------\n",
    "# A Sigmoid function\n",
    "#-------------------------------------------------\n",
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-(X)))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# A Recitfied Linear Unit\n",
    "#-------------------------------------------------\n",
    "def relu(X):\n",
    "    # Rectify activations below 0\n",
    "    X = np.maximum(np.zeros(np.shape(X)),X)\n",
    "    \n",
    "    #Recitfy activations above 1\n",
    "    X = np.minimum(np.ones(np.shape(X)),X)\n",
    "\n",
    "    return X\n",
    "\n",
    "#-------------------------------------------------\n",
    "# A Tanh \n",
    "#-------------------------------------------------\n",
    "def tanh(X):\n",
    "    t  =(np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "    return t\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Compute yhat, given X and paramters beta\n",
    "#-------------------------------------------------\n",
    "X          = np.random.uniform(low= -100, high = 100, size = (1000,1))\n",
    "beta       = np.random.normal(size = (np.shape(X)[1] + 1,1))\n",
    "yhat_sig   = sigmoid(linear(X,beta))\n",
    "yhat_relu  = relu(linear(X,beta))\n",
    "yhat_tanh  = tanh(linear(X,beta))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Plot\n",
    "#-------------------------------------------------\n",
    "index  = np.argsort(X[:,0])\n",
    "X, yhat_sig, yhat_relu, yhat_tanh = X[index,:], yhat_sig[index,:], yhat_relu[index,:], yhat_tanh[index,:]\n",
    "plt.plot(X,yhat_sig,label=\"Sigmoid\")\n",
    "plt.plot(X,yhat_relu,label=\"ReLU\")\n",
    "plt.plot(X,yhat_tanh,label=\"tanh\")\n",
    "plt.xlabel('X');plt.ylabel('yhat')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the $sigmoid$, $ReLU$ and $tanh$ are among the most common activation functions, there are others too! You can learn more about the varieties of activation function, and their pros/cons [here](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html).\n",
    "\n",
    "## An LSTM in PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trickiest part of using PyTorch (in my opinion) is getting used to how to format your data before passing it to the model. In the case of RNNs there are special formatting considerations that result from the fact that we have sequences of multiple lengths we need to classify. The best way to understand this is to go through a simple example.\n",
    "\n",
    "### Step 1: Formatting Vocabualry:\n",
    "To begin, let's import the vocabulary we generated when batch processing the larger Rotten Tomatoes dataset from part 3 using a helper function I've written in `utils.prepareVocabulary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding tokens for padding <pad> at position [0], and missing <missing> at position [1]\n",
      "Number of words: 19495\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "vocabulary = utils.prepareVocabulary(vocab_json = 'materials/data/rt_reviews/vocabulary.json', remove_less_than = 25)\n",
    "\n",
    "print('Number of words:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> The helper function puts the vocabulary in alphabetical order, and adds two new special \"words\" `<pad>` and `<missing>` that I will use to denote padding and missing words respectively; the `<missing>` token will replace any tokens that do not show up in the vocabulary, and the `<pad>` will be appended to the end of shorter sentences in our dataset to ensure that all sentences technically have the same number of \"words\". Note that the helper function allows us to remove words as a function of their frequency through the `remove_less_than` variable. Our frequency cut-off of 25 removes about 80% of the words, but 20,000 is still a pretty significant number!\n",
    "\n",
    "### Step 2: One Hot Coding\n",
    "To build a model, we need a way of representing our text numerically. When we were doing sentence-level processing we used bag-of-words, when we're doing word level-processing we use [one-hot-coding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/). Let's go through an example with some very simple data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The move was very good.',\n",
      " 'the movie was bad someverynonsensicalword',\n",
      " 'not good!']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# Some Sample sentences\n",
    "#----------------------------------------------------\n",
    "sentences  = ['The move was very good.',                        \n",
    "              'the movie was bad someverynonsensicalword',     \n",
    "              'not good!']                                     \n",
    "\n",
    "\n",
    "pprint(sentences)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> As usually, let's start by tokenizing the text (using `nltk`) and doing some very simple pre-processing (i.e. converting all text to lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'move', 'was', 'very', 'good', '.'],\n",
      " ['the', 'movie', 'was', 'bad', 'someverynonsensicalword'],\n",
      " ['not', 'good', '!']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# Tokenize the sentences\n",
    "#----------------------------------------------------\n",
    "for i,sentence in enumerate(sentences):\n",
    "    sentences[i] = nltk.word_tokenize(gensim.utils.to_unicode(sentence.lower()))\n",
    "    \n",
    "pprint(sentences)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Now that we have the tokens, we can one-hot encode the sentences; that is, we will represent each word as it's index in the vocabulary. I've written a helper function for this in `utils.onehot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17308, 11292, 18830, 18566, 7499, 61],\n",
      " [17308, 11298, 18830, 1509, 1],\n",
      " [11733, 7499, 2]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# One hot code\n",
    "#----------------------------------------------------\n",
    "onehot_sentences = utils.onehot(list_of_tokenized_sentences = sentences, vocabulary = vocabulary)\n",
    "\n",
    "pprint(onehot_sentences)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each token that had a match in the vocabulary was converted to it's index in the vocabulary; for instance, the word `the` is the 17,308th word in the vocabulary. Also note that when a word does not show up in the vocabulary - such as `someverynonsensicalword` - it is assigned an index of 1. this is because our `utils.prepareVocabulary` helper function from earlier added a special  token `<missing>` as a catch-all for any missing tokens in the text, and assigned it an index of 1. \n",
    "\n",
    "\n",
    "### Step 3: Padding and Tensor Preparation\n",
    "As we discussed in the `Doc2Vec` portion of the tutorial, most models (including networks) will expect a fixed length input tensor. But in their current state, each of our simple sentences are different lengths because each has a different number of words! One way to handle this is to pad the shorter sentences with the `<pad>` token until all the sentences are the same length. Recall that the `utils.prepareVocabulary` helper function from earlier denoted a special token `<pad>` for this exact purpose, and assigned it at index 0. Hence, I can zero-pad the tensors to indicate the padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence lengths: tensor([6, 5, 3])\n",
      "tensor([[17308, 11292, 18830, 18566,  7499,    61],\n",
      "        [17308, 11298, 18830,  1509,     1,     0],\n",
      "        [11733,  7499,     2,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# Pad sentences so they are the same length\n",
    "#----------------------------------------------------\n",
    "seq_lengths = torch.LongTensor(list(map(len, onehot_sentences)))\n",
    "print('sequence lengths:',seq_lengths)\n",
    "\n",
    "seq_tensor = Variable(torch.zeros((len(onehot_sentences), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(onehot_sentences, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    \n",
    "print(seq_tensor)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> And now all of our sentences are the same length, excellent. Note that the above code block also took care of casting the sentences to tensors but there is one remaining step to use these tensors as inputs to PyTorch - we need to transpose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17308, 17308, 11733],\n",
      "        [11292, 11298,  7499],\n",
      "        [18830, 18830,     2],\n",
      "        [18566,  1509,     0],\n",
      "        [ 7499,     1,     0],\n",
      "        [   61,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Note, must be transposed or Pytorch complains.\n",
    "seq_tensor = seq_tensor.transpose(0,1)\n",
    "print(seq_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Why do we need to transpose them? There is no special reason other than Pytorch expects it that way by default. So, now we finally have we have everything in the right format for passing to a PyTorch neural network. Let's combine all these transformation steps of the data needed to convert the raw sentences to tensors, along with the `utils.getBatch` function from part 3, so that we have a end-to-end way to get batches of properly formatted tensor data from our larger dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTensorBatch(data_path, batch_number, batch_size, random_seed, max_sequence_length):\n",
    "    print(data_path)\n",
    "    #-----------------------------------------------------\n",
    "    # Import a batch of data\n",
    "    #-----------------------------------------------------\n",
    "    data, end_flag = utils.getBatch(data_path,  batch_number, batch_size, random_seed)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Unpack the JSON\n",
    "    #-----------------------------------------------------\n",
    "    reviews,freshness  = [], []\n",
    "    for row in data:\n",
    "        reviews.append(row['Review'])\n",
    "        freshness.append(row['Freshness'])\n",
    "    freshness = [float(x) for x in freshness]\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Format the outcome tensor - y\n",
    "    #-----------------------------------------------------\n",
    "    y = torch.tensor(freshness)\n",
    "    y = y.to(torch.float)\n",
    "    y = y.view(np.shape(y)[0] , 1)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Format the input tensor - X\n",
    "    #-----------------------------------------------------\n",
    "    for i,sentence in enumerate(reviews):\n",
    "        reviews[i] = nltk.word_tokenize(gensim.utils.to_unicode(sentence.lower()))\n",
    "    onehot_sentences = utils.onehot(list_of_tokenized_sentences = reviews, vocabulary = vocabulary)\n",
    "\n",
    "    seq_lengths = torch.LongTensor(list(map(len, onehot_sentences)))\n",
    "    seq_tensor  = Variable(torch.zeros((len(onehot_sentences), max_sequence_length))).long()\n",
    "\n",
    "    for idx, (seq, seqlen) in enumerate(zip(onehot_sentences, seq_lengths)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    seq_tensor = seq_tensor.transpose(0,1)\n",
    "\n",
    "    return seq_tensor, seq_lengths, y, end_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Finally, let's test out our `getTensorBatch` function above works by fetching a small batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "materials/data/rt_reviews/train_re_reviews.jsonl\n",
      "X: tensor([[11253, 17374,  7985],\n",
      "        [11896,  9286,   305],\n",
      "        [17308,   305, 10282],\n",
      "        [ 9458, 12366, 17547],\n",
      "        [14369,   295, 14880],\n",
      "        [ 1179, 18530,   340],\n",
      "        [17308,  9286,   859],\n",
      "        [  652, 11436,    38],\n",
      "        [11896,     1,  5826],\n",
      "        [17308,     1,   918],\n",
      "        [ 2931, 19400,  2836],\n",
      "        [  918,  8620,  4198],\n",
      "        [  897, 17340,    57],\n",
      "        [12170,  7823,   918],\n",
      "        [ 4623,  1722,  9301],\n",
      "        [11965, 11222, 17208],\n",
      "        [15732, 11896,   305],\n",
      "        [ 8505,  9301, 12938],\n",
      "        [10472,    57, 16509],\n",
      "        [17374,   918,  8787],\n",
      "        [  305,  8620, 17308],\n",
      "        [14542, 17305, 13319],\n",
      "        [  918, 11222,    61],\n",
      "        [15788,  7823,     8],\n",
      "        [    1,  1722,     0],\n",
      "        [11896,   305,     0],\n",
      "        [  305, 10174,     0],\n",
      "        [11298, 19100,     0],\n",
      "        [   61, 12046,     0],\n",
      "        [  301,     1,     0],\n",
      "        [ 7085,    57,     0],\n",
      "        [14347,  9301,     0],\n",
      "        [ 8787, 13307,     0],\n",
      "        [16084, 19300,     0],\n",
      "        [  302,  8010,     0],\n",
      "        [    0,  1722,     0],\n",
      "        [    0,  1868,     0],\n",
      "        [    0,   304,     0]])\n",
      "\n",
      "\n",
      "y: tensor([[0.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "data_path     = 'materials/data/rt_reviews/train_re_reviews.jsonl'\n",
    "X, X_lengths, y, end_flag = getTensorBatch(data_path, batch_number = 1, batch_size = 3, random_seed = 1, max_sequence_length=38)\n",
    "\n",
    "print('X:',X)\n",
    "print('\\n')\n",
    "print('y:',y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Note that the `getTensorBatch` function has as an input parameter the `max_sequence_length` - which indicates the length of the longest sequence in our dataset. I've placed this as `38` just for illustrative purposes, but you would need to identify the actual length from the data itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Pre-trained Embeddings\n",
    "The one-hot coded \"vectors\" are not really vectors yet - they are still just indicies! In order to use them for machine learning, we'll need to convert them to proper vectors. Instead of training these vectors from scratch, We can use pre-trained embeddings as the \"starting point\" for our word vectors. The model will be able to tune these embeddings to be best suited for the task. In the earlier portions of this tutorial, we used `gensim` to train our `word2vec` and `doc2vec` models. `Pytorch` has it's own embedding functionality and, importantly, the embeddings can be combined with other layers directly to train models end-to-end. Let's convert the `glove` embeddings into `tensor` format using my helper function `utils.loadEmbeddings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glove embeddings and arrange the vector columns according to the word indicies\n",
    "importlib.reload(utils)\n",
    "glove_embeddings = utils.loadEmbeddings(path          = 'materials/data/glove.6B.50d.txt', \n",
    "                                        vocabulary    = vocabulary, \n",
    "                                        embedding_dim = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The helper function sorts the vectors in the same order as the vocabulary and casts it to tensor format. That is, the one-hot indicies from above can be converted into vectors in the embedding matrix! Here's an example with the word `apple`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5204, -0.8314,  0.4996,  1.2893,  0.1151,  0.0575, -1.3753, -0.9731,\n",
      "         0.1835,  0.4767, -0.1511,  0.3553,  0.2591, -0.7786,  0.5218,  0.4769,\n",
      "        -1.4251,  0.8580,  0.5982, -1.0903,  0.3357, -0.6089,  0.4174,  0.2157,\n",
      "        -0.0742, -0.5822, -0.4502,  0.1725,  0.1645, -0.3841,  2.3283, -0.6668,\n",
      "        -0.5818,  0.7439,  0.0950, -0.4787, -0.8459,  0.3870,  0.2369, -1.5523,\n",
      "         0.6480, -0.1652, -1.4719, -0.1622,  0.7986,  0.9739,  0.4003, -0.2191,\n",
      "        -0.3094,  0.2658])\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the word apple\n",
    "apple_index = vocabulary.index('apple')\n",
    "\n",
    "# Get the vector at that index\n",
    "print(glove_embeddings[apple_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> Now that our data (the hard part) is ready to go - we can finally specify the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model Specification\n",
    "Let's specify the RNN in torch. I've included ample comments within this function in an effort to explain the logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    #define all the layers used in model\n",
    "    \n",
    "    # Whenever an instance of a class is created, init function is automatically invoked. \n",
    "    # We will define all the layers that we will be using in the model\n",
    "    def __init__(self, vocab_size, embedding_dim, embeddings, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()          \n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding        = nn.Embedding(vocab_size, embedding_dim)  # This is a simple lookup table that turns the one-hot encodings into a vector - just like word2vec!\n",
    "        self.embedding.weight = nn.Parameter(embeddings)                 # pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "        \n",
    "        # lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,                        # Dimension of input\n",
    "                            hidden_dim,                           # Number of hidden nodes\n",
    "                            num_layers    = n_layers,             # Number of layers to be stacked\n",
    "                            bidirectional = bidirectional,        # If True, uses a Bi directional LSTM\n",
    "                            dropout       = dropout               # Introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout.\n",
    "                           )\n",
    "        \n",
    "        # dense layer\n",
    "        multiplier = 1 if bidirectional == False else 2\n",
    "        self.fc    = nn.Linear(hidden_dim * multiplier, output_dim)\n",
    "        \n",
    "        # final activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    # This is where we chain together to layers to get the output we want\n",
    "    def forward(self, text, text_lengths, bidirectional):\n",
    "        \n",
    "        # 1. Embedding: is used to convert the one-hot word representations into their\n",
    "        #               Word vector representations. \n",
    "        embedded = self.embedding(text)\n",
    "      \n",
    "        # 2. Pack padding: tells the network to ignore the inputs that are `<pad>`. \n",
    "        #                  so that the outputs we generate are not influenced by the `<pad>` tokens; \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n",
    "        \n",
    "        # 3. LSTM: is a variant of RNN that captures long term dependencies.\n",
    "        #          Following some important parameters of LSTM that you should be familiar with. Given below are the parameters of this layer:Run the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "      \n",
    "        # 4. Concatenate: the final forward and backward hidden state\n",
    "        hidden = hidden[-1,:,:] if bidirectional == False else torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # 5. Linear: Connect to a densely connected linear layer\n",
    "        dense_outputs =self.fc(hidden)\n",
    "            \n",
    "\n",
    "        # 6. ACtivation: Pass value to our sigmoid \n",
    "        outputs=self.act(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Now that the model is specified, we can choose the settings of the hyper-parameters, optimizer and criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "import torch.optim as optim\n",
    "\n",
    "#-------------------------------------------------\n",
    "# setting hyperparameters\n",
    "#-------------------------------------------------\n",
    "data_path          = 'materials/data/rt_reviews/train_re_reviews.jsonl'\n",
    "num_hidden_nodes   = 32\n",
    "num_output_nodes   = 1\n",
    "num_layers         = 2\n",
    "bidirectional      = False\n",
    "dropout            = 0.2\n",
    "size_of_vocab      = len(vocabulary)\n",
    "embeddings         = glove_embeddings\n",
    "embedding_dim      = glove_embeddings.size()[1]\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Instantiate the model\n",
    "#-------------------------------------------------\n",
    "model = classifier(size_of_vocab, embedding_dim,embeddings,num_hidden_nodes,\n",
    "                   num_output_nodes, num_layers, bidirectional = bidirectional, dropout = dropout)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Set the optimizer\n",
    "#-------------------------------------------------\n",
    "optimizer       = optim.Adam(model.parameters())\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Set the loss criterion\n",
    "#-------------------------------------------------\n",
    "criterion       = nn.BCELoss()\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Optionally, push to GPU, if available\n",
    "#-------------------------------------------------\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model     = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>One thing that I like to do to make sure my model is sensible is to print out the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(19495, 50)\n",
      "  (lstm): LSTM(50, 32, num_layers=2, dropout=0.2)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Notice that the model takes in a 19,495 vector (these are the \"one-hot\" vectors) and converts it to a 50 dimensional representation (the glove embeddings). The model passes this 50 dimensional vector to the LSTM which outputs a 32 dimensional vector that is passed to a linear layer, and finally to a sigmoid. We can check how many parameters the model has too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 993,983 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "print(f'The model has {utils.count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Nearly a million! Now, as a final check before training, let's make sure that that the model works when we put data in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "materials/data/rt_reviews/train_re_reviews.jsonl\n",
      "tensor([0.5142, 0.5090, 0.5127], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Import a batch of data\n",
    "data_path     = 'materials/data/rt_reviews/train_re_reviews.jsonl'\n",
    "X, X_lengths, y, end_flag = getTensorBatch(data_path, \n",
    "                                           batch_number        = 1, \n",
    "                                           batch_size          = 3,\n",
    "                                           random_seed         = 1, \n",
    "                                           max_sequence_length = 50)\n",
    "\n",
    "# Make sure that the model works by passing data into it, and seeing what comes out.\n",
    "predictions = model(X, X_lengths, bidirectional).squeeze()        \n",
    "\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> We got three tensors back, one for each data-point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 4: \n",
    "#### Worth 1/5 Points\n",
    "#### A. Batch Training the network\n",
    "Write a function to `train` the LSTM `classifier` specified in the tutorial, above. Note that the procedure to train the model here will be almost identical to what we did in Homework 3. More specifically, your train function should allow you to:\n",
    "\n",
    "1. Train the model for multiple `epochs`, \n",
    "2. Break each epoch's training into multiple `batches`, \n",
    "3. generate your `predictions` (i.e. `model(X, X_lengths).squeeze()`) on each batch,\n",
    "4. compute the loss using the batch predictions and the batch labels (`y`).\n",
    "5. back-propagate the batch loss and compute gradients\n",
    "6. Update the model parameters\n",
    "8. Stop training when the validation set performance stops improving \n",
    "\n",
    "Prove that your train function works by training the `classifier` for several batches and plotting the loss of the model on the training and validation sets. You are welcome to tweak the settings of the hyper-parameters, add or subtract layers, etc. as long as you use an LSTM \n",
    "\n",
    "**Please Note:** In all likelihood your personal machines will not have the computational might (or time) required to train a perfect LSTM model with 1M parameters. You are not required to build a high-performance model here, rather, I just want to see that you can train and evaluate the model *in batches*. While it is not required, you can use the [MSU High Performance Computing Cluster](https://icer.msu.edu/users/getting-startedhttps://icer.msu.edu/users/getting-started) if you want to explore a greater variety of network settings, or improve the performance of the networks by exploring the hyper-parameters more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise: \n",
    "    * <span style=\"color:red\">X</span>/1 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
